"""
ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“ˆ (Polars ê¸°ë°˜)
DB íŒŒì¼ ì½ê¸°, PLC ë³µì›, CNT ë°ì´í„° í•„í„°ë§, êµ¬ê°„ ë¶„ì„ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
Polarsë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, matplotlib í˜¸í™˜ì„ ìœ„í•´ í•„ìš”í•œ ë¶€ë¶„ë§Œ pandasë¡œ ë³€í™˜
"""

import sqlite3
try:
    from print_utils import tprint
except ImportError:
    # print_utilsê°€ ì—†ìœ¼ë©´ ì¼ë°˜ print ì‚¬ìš©
    def tprint(*args, **kwargs):
        import datetime
        timestamp = datetime.datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
        message_parts = [timestamp] + [str(arg) for arg in args]
        message = " ".join(message_parts)
        if 'sep' not in kwargs:
            kwargs['sep'] = ' '
        if 'end' not in kwargs:
            kwargs['end'] = '\n'
        print(message, **kwargs)
import polars as pl  # PolarsëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •
POLARS_AVAILABLE = True

# matplotlib í˜¸í™˜ì„ ìœ„í•œ pandas (ìµœì†Œí•œë§Œ ì‚¬ìš©)
import pandas as pd
import numpy as np
import re
import datetime
import os
import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import tkinter as tk
from tkinter import ttk, messagebox
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
import hashlib
import pickle


def extract_date_from_filename(filename):
    """
    íŒŒì¼ëª…ì—ì„œ YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œ ì¶”ì¶œ
    
    Args:
        filename: íŒŒì¼ëª… (ê²½ë¡œ í¬í•¨ ê°€ëŠ¥)
        
    Returns:
        datetime.datetime: ì¶”ì¶œëœ ë‚ ì§œ, ì‹¤íŒ¨ ì‹œ None
    """
    match = re.search(r"(\d{4})-(\d{2})-(\d{2})", os.path.basename(filename))
    if match:
        try:
            year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            return datetime.datetime(year, month, day)
        except ValueError:
            return None
    return None


# ============================================================================
# 1. ë²¡í„°í™”ëœ DateTime ë³€í™˜ ì•Œê³ ë¦¬ì¦˜ (Polars ê¸°ë°˜)
# ============================================================================

def convert_datetime_vectorized_polars(lf_or_df, time_col, base_date):
    """
    Polars DataFrame/LazyFrameì„ ì‚¬ìš©í•œ ë²¡í„°í™” datetime ë³€í™˜
    
    Args:
        lf_or_df: Polars DataFrame ë˜ëŠ” LazyFrame
        time_col: ì‹œê°„ ì»¬ëŸ¼ëª…
        base_date: ê¸°ë³¸ ë‚ ì§œ (datetime ê°ì²´)
        
    Returns:
        Polars LazyFrame: datetime ì»¬ëŸ¼ì´ ì¶”ê°€ëœ LazyFrame
    """
    try:
        # base_dateë¥¼ Polars datetime literalë¡œ ìƒì„±
        base_datetime = datetime.datetime(
            base_date.year, base_date.month, base_date.day,
            0, 0, 0
        )
        base_date_pl = pl.lit(base_datetime).cast(pl.Datetime)
        
        # ìŠ¤í‚¤ë§ˆì—ì„œ íƒ€ì… í™•ì¸ (LazyFrameì¸ ê²½ìš° collect_schema() ì‚¬ìš©)
        if hasattr(lf_or_df, 'collect_schema'):
            # LazyFrame: collect_schema()ë¡œ ê²½ëŸ‰ ìŠ¤í‚¤ë§ˆ í™•ì¸
            schema = lf_or_df.collect_schema()
            col_dtype = schema.get(time_col)
        elif hasattr(lf_or_df, 'schema'):
            # DataFrame: ì§ì ‘ schema ì†ì„± ì‚¬ìš©
            schema = lf_or_df.schema
            col_dtype = schema.get(time_col) if hasattr(schema, 'get') else None
        else:
            col_dtype = None
        
        if col_dtype == pl.Datetime:
            # datetime íƒ€ì…: ë‚ ì§œëŠ” base_date, ì‹œê°„ë§Œ ì¶”ì¶œí•˜ì—¬ ë”í•˜ê¸°
            # ì‹œê°„ì„ ì´ˆë¡œ ë³€í™˜ (ë²¡í„°í™”)
            time_seconds = (
                pl.col(time_col).dt.hour() * 3600
                + pl.col(time_col).dt.minute() * 60
                + pl.col(time_col).dt.second()
                + pl.col(time_col).dt.microsecond() / 1_000_000  # ë§ˆì´í¬ë¡œì´ˆë¥¼ ì´ˆë¡œ ë³€í™˜
            )
            datetime_expr = base_date_pl + pl.duration(seconds=time_seconds)
        else:
            # ìˆ«ì íƒ€ì…: ì´ˆ ë‹¨ìœ„ë¡œ í•´ì„í•˜ì—¬ base_dateì— ë”í•˜ê¸°
            datetime_expr = (
                base_date_pl 
                + pl.duration(seconds=pl.col(time_col).cast(pl.Int64))
            )
        
        return lf_or_df.with_columns([
            datetime_expr.alias("datetime")
        ])
        
    except Exception as e:
        print(f"Polars datetime ë³€í™˜ ì‹¤íŒ¨, pandas fallback ì‚¬ìš©: {e}")
        # fallback: pandasë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
        if hasattr(lf_or_df, 'collect'):
            df_pd = lf_or_df.collect().to_pandas()
        elif hasattr(lf_or_df, 'to_pandas'):
            df_pd = lf_or_df.to_pandas()
        else:
            df_pd = lf_or_df
            
        df_pd['datetime'] = convert_datetime_vectorized(df_pd[time_col], base_date)
        return pl.from_pandas(df_pd).lazy()


# ============================================================================
# 2. PLC Error ê¸°ë°˜ NaN ë³µì› ì•Œê³ ë¦¬ì¦˜ (Polars ê¸°ë°˜)
# ============================================================================

def restore_plc_error_data_polars(lf, plc_error_col, cols_in_db):
    """
    Polars ê¸°ë°˜ PLC error ì •ë³´ë¥¼ ì‚¬ìš©í•œ NaN ë°ì´í„° ë³µì›
    
    Args:
        lf: Polars LazyFrame
        plc_error_col: PLC error ì»¬ëŸ¼ëª…
        cols_in_db: ë³µì›í•  íŒŒë¼ë¯¸í„° ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        Polars LazyFrame: ë³µì›ëœ LazyFrame (collectëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ)
    """
    # LazyFrame ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì„±ëŠ¥ ê²½ê³  ë°©ì§€)
    schema_names = lf.collect_schema().names()
    if plc_error_col is None or plc_error_col not in schema_names:
        return lf
    
    tprint(f"PLC error ì´ˆê³ ì† ë³µì› ì‹œì‘ (Polars): {plc_error_col}")
    
    # 1. PLC ìƒíƒœ ë³µì›
    # ì²« ìœ íš¨ê°’ì„ ì°¾ê¸° ìœ„í•´ ë¨¼ì € collect (ì‘ì€ ì‘ì—…)
    df_temp = lf.select(pl.col(plc_error_col)).head(1000).collect()
    initial_state = 0
    if len(df_temp) > 0 and df_temp[plc_error_col].is_not_null().any():
        first_valid = df_temp.filter(pl.col(plc_error_col).is_not_null())
        if len(first_valid) > 0:
            initial_state = int(first_valid[plc_error_col][0])
    
    # PLC error ì»¬ëŸ¼ forward fill
    lf = lf.with_columns([
        pl.col(plc_error_col)
        .forward_fill()
        .fill_null(initial_state)
        .cast(pl.Int64)
        .alias(plc_error_col)
    ])
    
    # 2. ì—ëŸ¬ ë§ˆìŠ¤í¬ ìƒì„±
    error_mask = pl.col(plc_error_col) == 1
    valid_mask = pl.col(plc_error_col) == 0
    
    # 3. ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆì— ë³µì› (ë²¡í„°í™” ìµœì í™”)
    schema_names = lf.collect_schema().names()
    restore_columns = []
    
    for param in cols_in_db:
        if param in schema_names and param != plc_error_col:
            # ì—ëŸ¬ êµ¬ê°„ì€ nullë¡œ ë§ˆìŠ¤í‚¹í•˜ê³  forward fill
            filled_col = (
                pl.when(error_mask)
                .then(None)  # ì—ëŸ¬ êµ¬ê°„ì€ null
                .otherwise(pl.col(param))
                .forward_fill()
            )
            
            # ì •ìƒ êµ¬ê°„ì—ì„œë§Œ ë³µì›ëœ ê°’ ì‚¬ìš©
            restore_columns.append(
                pl.when(valid_mask & filled_col.is_not_null())
                .then(filled_col)
                .otherwise(pl.col(param))
                .alias(param)
            )
    
    # ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬ (ë‹¨ì¼ with_columns í˜¸ì¶œë¡œ ìµœì í™”)
    if restore_columns:
        lf = lf.with_columns(restore_columns)
    
    tprint(f"  ì²˜ë¦¬ ì™„ë£Œ (Polars, {len(restore_columns)}ê°œ íŒŒë¼ë¯¸í„°)")
    
    return lf


# ============================================================================
# 1. ë²¡í„°í™”ëœ DateTime ë³€í™˜ ì•Œê³ ë¦¬ì¦˜ (Pandas í˜¸í™˜ - fallbackìš©)
# ============================================================================

def make_to_datetime_safe(base_date):
    """
    ì•ˆì „í•œ datetime ë³€í™˜ í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Args:
        base_date: ê¸°ë³¸ ë‚ ì§œ (datetime ê°ì²´)
        
    Returns:
        to_datetime_safe í•¨ìˆ˜
    """
    def to_datetime_safe(value):
        # NaNì´ë‚˜ None ê°’ ë¨¼ì € ì²´í¬
        if pd.isna(value):
            return pd.NaT
            
        # isinstance ì²´í¬ ë¶€ë¶„ ìˆ˜ì • - datetime.datetimeìœ¼ë¡œ ë³€ê²½
        if isinstance(value, pd.Timestamp) or isinstance(value, datetime.datetime):
            time_part = value.time()
            return base_date.replace(hour=time_part.hour,
                                     minute=time_part.minute,
                                     second=time_part.second,
                                     microsecond=time_part.microsecond)
        try:
            # ë¬¸ìì—´ì´ë‚˜ ìˆ«ì íƒ€ì…ì„ ë¨¼ì € í™•ì¸
            if isinstance(value, str):
                # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                if not value.strip():
                    return pd.NaT
                value = float(value)
            elif not isinstance(value, (int, float)):
                return pd.NaT
                
            return base_date + datetime.timedelta(seconds=int(float(value)))
        except (ValueError, TypeError, OverflowError):
            return pd.NaT
    return to_datetime_safe


def convert_datetime_vectorized(series, base_date):
    """
    pandas Seriesë¥¼ ë²¡í„°í™” ë°©ì‹ìœ¼ë¡œ datetime ë³€í™˜
    
    Args:
        series: ë³€í™˜í•  pandas Series (ì‹œê°„ ë°ì´í„°)
        base_date: ê¸°ë³¸ ë‚ ì§œ (datetime ê°ì²´)
        
    Returns:
        pandas Series: datetime ë³€í™˜ëœ Series
    """
    try:
        # 1. NaN ê°’ ì²˜ë¦¬
        valid_mask = pd.notna(series)
        result = pd.Series(pd.NaT, index=series.index, dtype='datetime64[ns]')
        
        if not valid_mask.any():
            return result
            
        valid_series = series[valid_mask]
        
        # 2. ì´ë¯¸ datetime íƒ€ì…ì¸ ê²½ìš° (ë²¡í„°í™” ì²˜ë¦¬)
        datetime_mask = valid_series.apply(lambda x: isinstance(x, (pd.Timestamp, datetime.datetime)))
        if datetime_mask.any():
            # datetime íƒ€ì… ê°’ë“¤ì„ Seriesë¡œ ë³€í™˜ (dt accessor ì‚¬ìš©ì„ ìœ„í•´)
            datetime_series = pd.to_datetime(valid_series[datetime_mask])
            
            # numpy ê¸°ë°˜ìœ¼ë¡œ ì‹œê°„ ë¶€ë¶„ì„ timedeltaë¡œ ë³€í™˜
            time_seconds = (
                datetime_series.dt.hour * 3600
                + datetime_series.dt.minute * 60
                + datetime_series.dt.second
                + datetime_series.dt.microsecond / 1_000_000  # ë§ˆì´í¬ë¡œì´ˆë„ í¬í•¨
            )
            
            # base_dateì— ì‹œê°„ ë¶€ë¶„ì„ ë”í•´ì„œ ìµœì¢… datetime ìƒì„±
            result.loc[datetime_series.index] = pd.Timestamp(base_date) + pd.to_timedelta(time_seconds, unit='s')
        
        # 3. ìˆ«ì íƒ€ì… ì²˜ë¦¬ (vectorized)
        numeric_mask = ~datetime_mask
        if numeric_mask.any():
            numeric_values = valid_series[numeric_mask]
            try:
                # ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
                numeric_converted = pd.to_numeric(numeric_values, errors='coerce')
                valid_numeric = pd.notna(numeric_converted)
                
                if valid_numeric.any():
                    # ë²¡í„°í™”ëœ timedelta ê³„ì‚°
                    seconds = numeric_converted[valid_numeric].astype(int)
                    base_timestamps = pd.Timestamp(base_date)
                    result.loc[numeric_converted[valid_numeric].index] = base_timestamps + pd.to_timedelta(seconds, unit='s')
                    
            except Exception:
                pass
        
        return result
        
    except Exception as e:
        print(f"ë²¡í„°í™” ë³€í™˜ ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
        # fallback to apply method
        to_datetime_safe = make_to_datetime_safe(base_date)
        return series.apply(to_datetime_safe)


# ============================================================================
# 2. PLC Error ê¸°ë°˜ NaN ë³µì› ì•Œê³ ë¦¬ì¦˜ (Pandas í˜¸í™˜ - fallbackìš©)
# ============================================================================

def restore_plc_error_data(df, plc_error_col, cols_in_db):
    """
    PLC error ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ NaN ë°ì´í„°ë¥¼ ë³µì›í•˜ëŠ” í•¨ìˆ˜ (ì´ˆê³ ì† ì„¸ê·¸ë¨¼íŠ¸ ê¸°ë°˜)
    
    Args:
        df: ì²˜ë¦¬í•  DataFrame
        plc_error_col: PLC error ì»¬ëŸ¼ëª…
        cols_in_db: ë³µì›í•  íŒŒë¼ë¯¸í„° ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        DataFrame: ë³µì›ëœ DataFrame
    """
    if plc_error_col is None or plc_error_col not in df.columns:
        return df
    
    print(f"PLC error ì´ˆê³ ì† ë³µì› ì‹œì‘: {plc_error_col}")
    
    # 1. PLC ìƒíƒœ ë³µì› (fillna í•œ ë²ˆë§Œ ì‚¬ìš©)
    plc_raw = df[plc_error_col].copy()
    first_valid = plc_raw.first_valid_index()
    initial_state = 0 if first_valid is None else int(plc_raw.loc[first_valid])
    
    # ë§¤ìš° ë¹ ë¥¸ forward fill
    plc_restored = plc_raw.fillna(method='ffill').fillna(initial_state).astype(int)
    df[plc_error_col] = plc_restored
    
    # 2. ì—ëŸ¬ êµ¬ê°„ ì°¾ê¸° (ë²¡í„°í™”)
    error_mask = plc_restored == 1
    valid_mask = plc_restored == 0  # ì •ìƒ êµ¬ê°„ ë§ˆìŠ¤í¬
    
    # 3. ë²¡í„°í™” ê¸°ë°˜ ë³µì› (í•œ ë²ˆì— ì²˜ë¦¬)
    for param in cols_in_db:
        if param in df.columns and param != plc_error_col:
            original_nan_count = df[param].isna().sum()
            
            if original_nan_count > 0:
                # ì—ëŸ¬ êµ¬ê°„ì„ NaNìœ¼ë¡œ ë§ˆìŠ¤í‚¹í•˜ì—¬ forward fillì´ ì „íŒŒë˜ì§€ ì•Šë„ë¡ í•¨
                param_data = df[param].copy()
                param_data[error_mask] = np.nan
                
                # forward fill ì ìš© (ì—ëŸ¬ êµ¬ê°„ì€ NaNìœ¼ë¡œ ìœ ì§€ë˜ì–´ ì „íŒŒ ë°©ì§€)
                filled_data = param_data.fillna(method='ffill')
                
                # ì •ìƒ êµ¬ê°„ì—ì„œë§Œ ë³µì›ëœ ê°’ì„ ì‚¬ìš©
                # where: valid_maskê°€ False(ì—ëŸ¬)ì´ë©´ ì›ë˜ ê°’, True(ì •ìƒ)ì´ë©´ filled_data ì‚¬ìš©
                df[param] = df[param].where(~valid_mask, filled_data)
                
                restored_nan_count = df[param].isna().sum()
                restored_count = original_nan_count - restored_nan_count
                
                if restored_count > 0:
                    print(f"  {param}: {restored_count} í¬ì¸íŠ¸ ë³µì›")
    
    # ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜ ê³„ì‚° (ë¡œê¹…ìš©)
    error_diff = error_mask.astype(int).diff().fillna(0)
    error_start_indices = df.index[error_diff == 1].tolist()
    segments_count = len(error_start_indices)
    
    print(f"  ì²˜ë¦¬ ì™„ë£Œ: {segments_count + 1}ê°œ ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸ (ì¶”ì •)")
    print(f"  PLC error êµ¬ê°„: {error_mask.sum()} í¬ì¸íŠ¸")
    
    return df


# ============================================================================
# DB íŒŒì¼ ì½ê¸° í•¨ìˆ˜
# ============================================================================

def is_restored_file(file_path):
    """
    íŒŒì¼ëª…ì´ 'restored'ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
    
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        
    Returns:
        bool: íŒŒì¼ëª…ì´ 'restored'ë¡œ ëë‚˜ë©´ True
    """
    filename = os.path.basename(file_path)
    # í™•ì¥ì ì œê±° í›„ í™•ì¸
    name_without_ext = os.path.splitext(filename)[0]
    return name_without_ext.lower().endswith('restored')


def read_db_file(db_path, params_to_read, time_cols, convert_datetime_vectorized):
    """
    DB íŒŒì¼ ì½ê¸° í•¨ìˆ˜ - PLC ë³µì› ê¸°ëŠ¥ í¬í•¨ (Polars ê¸°ë°˜)
    
    Args:
        db_path: ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
        params_to_read: ì½ì„ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸
        time_cols: ì‹œê°„ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        convert_datetime_vectorized: ë²¡í„°í™”ëœ datetime ë³€í™˜ í•¨ìˆ˜ (í˜¸í™˜ì„± ìœ ì§€ìš©, ì‚¬ìš© ì•ˆ í•¨)
        
    Returns:
        pd.DataFrame: ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ (matplotlib í˜¸í™˜ì„ ìœ„í•´ pandasë¡œ ë°˜í™˜)
    """
    if not POLARS_AVAILABLE:
        raise ImportError("Polarsê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install polars")
    
    # íŒŒì¼ëª…ì´ 'restored'ë¡œ ëë‚˜ë©´ PLC ë³µì› ê±´ë„ˆë›°ê¸°
    skip_plc_restoration = is_restored_file(db_path)
    
    # SQLite ì—°ê²° ìµœì í™” ë° ìŠ¤í‚¤ë§ˆ í™•ì¸ (Polarsë¡œ ì§ì ‘)
    conn = None
    try:
        conn = sqlite3.connect(db_path)
        # SQLite ì„±ëŠ¥ ìµœì í™” PRAGMA ì„¤ì •
        conn.execute("PRAGMA journal_mode=OFF")  # WAL ë¹„í™œì„±í™” (ì½ê¸° ì „ìš© ì‹œ)
        conn.execute("PRAGMA synchronous=OFF")   # ë™ê¸°í™” ë¹„í™œì„±í™” (ì†ë„ í–¥ìƒ)
        conn.execute("PRAGMA cache_size=-100000")  # ìºì‹œ í¬ê¸° ì¦ê°€ (100MB)
        
        # Polarsë¡œ ì§ì ‘ ìŠ¤í‚¤ë§ˆ í™•ì¸ (pandas ê±°ì¹˜ì§€ ì•ŠìŒ)
        try:
            schema_df = pl.read_database(
                query="PRAGMA table_info(data)",
                connection=conn
            )
            available_cols = schema_df['name'].to_list()
        except Exception:
            # Polarsë¡œ ì‹¤íŒ¨ ì‹œ ìµœì†Œí•œë§Œ pandas ì‚¬ìš© (fallback)
            pragma_df = pd.read_sql_query("PRAGMA table_info(data)", conn)
            available_cols = pragma_df['name'].tolist()
    except Exception as e:
        print(f"{db_path} ìŠ¤í‚¤ë§ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
        if conn:
            conn.close()
        return None

    # datetime ì»¬ëŸ¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë³‘í•© íŒŒì¼ ì²˜ë¦¬)
    datetime_already_exists = 'datetime' in available_cols
    
    if datetime_already_exists:
        time_col = 'datetime'
    else:
        # time ì»¬ëŸ¼ ì°¾ê¸° (ê¸°ì¡´ íŒŒì¼)
        time_col = next((c for c in time_cols if c in available_cols), None)
    if time_col is None:
        return None
    
    cols_in_db = [col for col in params_to_read if col in available_cols]
    if not cols_in_db:
        return None
    
    # PLC error ì»¬ëŸ¼ ì°¾ê¸°
    plc_error_candidates = ['plc_connection_error', 'serverFault', 'fault']
    plc_error_col = None
    for candidate in plc_error_candidates:
        if candidate in available_cols:
            plc_error_col = candidate
            break
    
    query_cols = [time_col] + cols_in_db
    if plc_error_col and plc_error_col not in query_cols:
        query_cols.append(plc_error_col)
    
    query = f"SELECT {', '.join(query_cols)} FROM data"
    
    # Polarsë¡œ ì§ì ‘ SQLite ì½ê¸° (LazyFrameìœ¼ë¡œ - íš¨ìœ¨ì )
    lf = None
    
    # ë°©ë²• 1: sqlite3 ì—°ê²° ê°ì²´ ì‚¬ìš© (ìµœì í™”ëœ ì—°ê²° ì¬ì‚¬ìš©)
    try:
        if conn is None:
            conn = sqlite3.connect(db_path)
            conn.execute("PRAGMA journal_mode=OFF")
            conn.execute("PRAGMA synchronous=OFF")
            conn.execute("PRAGMA cache_size=-100000")
        
        # Polarsë¡œ ì§ì ‘ ì½ê¸° (LazyFrame ë°˜í™˜ ì‹œë„)
        df_temp = pl.read_database(
            query=query,
            connection=conn
        )
        # LazyFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì—°ì‚° ì²´ì´ë‹
        lf = df_temp.lazy()
        conn.close()
        conn = None
    except Exception as e:
        # ë°©ë²• 2: connection_uri ì‚¬ìš©
        try:
            if conn:
                conn.close()
                conn = None
            df_temp = pl.read_database_uri(
                uri=f"sqlite:///{db_path}",
                query=query,
                engine="connectorx"
            )
            lf = df_temp.lazy()
        except Exception as e2:
            # ë°©ë²• 3: pandasë¡œ ì½ì–´ì„œ Polarsë¡œ ë³€í™˜ (ìµœì¢… fallback)
            try:
                if conn is None:
                    conn = sqlite3.connect(db_path)
                df_pd_temp = pd.read_sql_query(query, conn)
                if conn:
                    conn.close()
                    conn = None
                df_temp = pl.from_pandas(df_pd_temp)
                lf = df_temp.lazy()
            except Exception as e3:
                print(f"{db_path} ì½ê¸° ì‹¤íŒ¨: {e3}")
                if conn:
                    conn.close()
                return None
    
    # ì»¬ëŸ¼ íƒ€ì… ë³€í™˜ì„ LazyFrame ë‹¨ê³„ë¡œ ì´ë™ (í•œ ë²ˆì— ì²˜ë¦¬)
    # LazyFrameì—ì„œ ìŠ¤í‚¤ë§ˆ í™•ì¸ (ê²½ëŸ‰ ì‘ì—…)
    schema = lf.collect_schema()
    type_conversions = []
    
    for col in cols_in_db:
        if col in schema:
            col_dtype = schema[col]
            # Objectë‚˜ Utf8 íƒ€ì…ì´ë©´ Float64ë¡œ ë³€í™˜
            if col_dtype == pl.Object or col_dtype == pl.Utf8:
                type_conversions.append(
                    pl.col(col).cast(pl.Float64, strict=False).alias(col)
                )
    
    # íƒ€ì… ë³€í™˜ì„ í•œ ë²ˆì— ì ìš© (LazyFrame ìœ ì§€)
    if type_conversions:
        lf = lf.with_columns(type_conversions)
    
    # datetime ì»¬ëŸ¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë³€í™˜ ê³¼ì • ìƒëµ (LazyFrame ì²´ì´ë‹ ìœ ì§€)
    if datetime_already_exists:
        # datetime ì»¬ëŸ¼ì„ Polars datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (LazyFrame ë‹¨ê³„)
        try:
            lf = lf.with_columns(
                pl.col('datetime').str.to_datetime().alias('datetime')
            )
        except Exception:
            # ì´ë¯¸ datetime íƒ€ì…ì´ê±°ë‚˜ ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš°
            try:
                lf = lf.with_columns(
                    pl.col('datetime').cast(pl.Datetime).alias('datetime')
                )
            except Exception:
                pass  # ë³€í™˜ì´ í•„ìš” ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
        
        # PLC error ê¸°ë°˜ NaN ë³µì› (LazyFrame ì²´ì´ë‹ ìœ ì§€)
        # collect_schema()ëŠ” ìŠ¤í‚¤ë§ˆë§Œ í™•ì¸í•˜ë¯€ë¡œ ê²½ëŸ‰ ì‘ì—…
        schema_names = lf.collect_schema().names()
        if plc_error_col and plc_error_col in schema_names and not skip_plc_restoration:
            lf = restore_plc_error_data_polars(lf, plc_error_col, cols_in_db)
        elif skip_plc_restoration:
            tprint(f"  íŒŒì¼ëª…ì´ 'restored'ë¡œ ëë‚˜ë¯€ë¡œ PLC ë³µì›ì„ ê±´ë„ˆëœë‹ˆë‹¤: {os.path.basename(db_path)}")
        
    else:
        # ê¸°ì¡´ ë¡œì§: time ì»¬ëŸ¼ì—ì„œ datetime ìƒì„± (ì›ë³¸ íŒŒì¼ ì²˜ë¦¬)
        # ë‚ ì§œ ì¶”ì¶œ
        base_date = extract_date_from_filename(db_path)
        if base_date is None:
            tprint(f"ê²½ê³ : {os.path.basename(db_path)} íŒŒì¼ëª…ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            base_date = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        # datetime ë³€í™˜ (LazyFrame ì²´ì´ë‹ ìœ ì§€)
        lf = convert_datetime_vectorized_polars(lf, time_col, base_date)
        
        # PLC error ê¸°ë°˜ NaN ë³µì› (LazyFrame ì²´ì´ë‹ ìœ ì§€)
        schema_names = lf.collect_schema().names()
        if plc_error_col and plc_error_col in schema_names and not skip_plc_restoration:
            lf = restore_plc_error_data_polars(lf, plc_error_col, cols_in_db)
        elif skip_plc_restoration:
            tprint(f"  íŒŒì¼ëª…ì´ 'restored'ë¡œ ëë‚˜ë¯€ë¡œ PLC ë³µì›ì„ ê±´ë„ˆëœë‹ˆë‹¤: {os.path.basename(db_path)}")
    
    # LazyFrame ì‹¤í–‰ - ë§ˆì§€ë§‰ì— í•œ ë²ˆë§Œ collect() í˜¸ì¶œ
    df_pl_result = lf.collect()
    
    # matplotlib í˜¸í™˜ì„ ìœ„í•´ pandasë¡œ ë³€í™˜ (ë§ˆì§€ë§‰ ë‹¨ê³„)
    return df_pl_result.to_pandas()


def read_multiple_db_files_parallel(db_files, params_to_read, time_cols, convert_datetime_vectorized, 
                                     max_workers=None, skip_cnt_check=False):
    """
    ì—¬ëŸ¬ DB íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì½ê¸° (ThreadPoolExecutor + Polars ë³‘ë ¬ ì²˜ë¦¬)
    
    ThreadPoolExecutorë¡œ ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì½ê³ , ê° íŒŒì¼ ë‚´ë¶€ì˜ ë°ì´í„° ì²˜ë¦¬(PLC ë³µì›, ë³€í™˜ ë“±)ëŠ” Polarsê°€ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        db_files: ì½ì„ DB íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        params_to_read: ì½ì„ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸
        time_cols: ì‹œê°„ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        convert_datetime_vectorized: ë²¡í„°í™”ëœ datetime ë³€í™˜ í•¨ìˆ˜
        max_workers: ìµœëŒ€ ë³‘ë ¬ ì‘ì—… ìˆ˜ (Noneì´ë©´ ìë™ ê²°ì •)
        skip_cnt_check: CNT ì²´í¬ ê±´ë„ˆë›°ê¸° ì—¬ë¶€
        
    Returns:
        list: ì½ì€ DataFrame ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨í•œ íŒŒì¼ì€ None)
    """
    
    if not db_files:
        return []
    
    # ìµœì ì˜ ì›Œì»¤ ìˆ˜ ê²°ì •
    # I/O ë°”ìš´ë“œ ì‘ì—…(íŒŒì¼ ì½ê¸°)ì´ë¯€ë¡œ ë…¼ë¦¬ í”„ë¡œì„¸ì„œ ìˆ˜ í™œìš©
    if max_workers is None:
        import multiprocessing
        logical_processors = multiprocessing.cpu_count()  # ë…¼ë¦¬ í”„ë¡œì„¸ì„œ ìˆ˜
        physical_cores = logical_processors // 2  # ëŒ€ëµì ì¸ ë¬¼ë¦¬ ì½”ì–´ ìˆ˜ (í•˜ì´í¼ìŠ¤ë ˆë”© ê³ ë ¤)
        
        # I/O ë°”ìš´ë“œ ì‘ì—…(íŒŒì¼ ì½ê¸°)ì€ I/O ëŒ€ê¸° ì‹œê°„ì´ ë§ìœ¼ë¯€ë¡œ
        # ë…¼ë¦¬ í”„ë¡œì„¸ì„œ ìˆ˜ì˜ 1.5ë°°ê°€ ì ì ˆ
        # ë…¼ë¦¬ í”„ë¡œì„¸ì„œ ìˆ˜ì— ë”°ë¼ ìµœëŒ€ê°’ ë™ì  ì¡°ì •:
        # - 8ê°œ ì´í•˜ (ì €ì‚¬ì–‘): ìµœëŒ€ ë…¼ë¦¬ í”„ë¡œì„¸ì„œ ìˆ˜ì˜ 2ë°° (ì˜ˆ: 8ê°œ â†’ ìµœëŒ€ 16ê°œ)
        # - 16ê°œ ì´ìƒ (ê³ ì‚¬ì–‘): ìµœëŒ€ 32ê°œë¡œ ì œí•œ
        recommended_workers = logical_processors + logical_processors // 2  # ë…¼ë¦¬ í”„ë¡œì„¸ì„œ ìˆ˜ì˜ 1.5ë°°
        
        # ìµœëŒ€ê°’ ë™ì  ì¡°ì •
        if logical_processors <= 8:
            # ì €ì‚¬ì–‘ CPU (i5-1135G7 ë“±): ë…¼ë¦¬ í”„ë¡œì„¸ì„œì˜ 2ë°°ê¹Œì§€
            max_recommended = logical_processors * 2  # 8ê°œ â†’ ìµœëŒ€ 16ê°œ
        else:
            # ê³ ì‚¬ì–‘ CPU (i7-13700 ë“±): ìµœëŒ€ 32ê°œë¡œ ì œí•œ
            max_recommended = 32
        
        recommended_workers = min(recommended_workers, max_recommended)
        recommended_workers = max(4, recommended_workers)  # ìµœì†Œ 4ê°œ
        max_workers = min(len(db_files), recommended_workers)
        
        # ì„¤ì • ì •ë³´ ì¶œë ¥
        tprint(f"  CPU ì •ë³´: {logical_processors} ë…¼ë¦¬ í”„ë¡œì„¸ì„œ, {physical_cores} ë¬¼ë¦¬ ì½”ì–´ (ì¶”ì •)")
        tprint(f"  ê¶Œì¥ ì›Œì»¤ ìˆ˜: {recommended_workers}ê°œ (I/O ë°”ìš´ë“œ ì‘ì—… ìµœì í™”)")
    
    results = {}
    
    def read_single_file(db_path):
        """ë‹¨ì¼ íŒŒì¼ ì½ê¸° (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
        try:
            # CNT ì²´í¬
            if not skip_cnt_check and is_cnt_related_data(db_path, params_to_read):
                return db_path, None, "CNT ê´€ë ¨ ë°ì´í„° ì œì™¸"
            
            # Polarsê°€ ë‚´ë¶€ì ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬í•˜ì—¬ íŒŒì¼ ì½ê¸° ë° PLC ë³µì› ìˆ˜í–‰
            df = read_db_file(db_path, params_to_read, time_cols, convert_datetime_vectorized)
            if df is not None:
                return db_path, df, "ì„±ê³µ"
            else:
                return db_path, None, "ì½ê¸° ì‹¤íŒ¨"
        except Exception as e:
            return db_path, None, f"ì˜¤ë¥˜: {str(e)}"
    
    # ë³‘ë ¬ë¡œ íŒŒì¼ ì½ê¸°
    tprint(f"  ì„¤ì •: ìµœëŒ€ {max_workers}ê°œ ìŠ¤ë ˆë“œë¡œ {len(db_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ (ê° íŒŒì¼ ë‚´ë¶€ëŠ” Polarsê°€ ë³‘ë ¬ ì²˜ë¦¬)")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  íŒŒì¼ ì½ê¸° ì‘ì—… ì œì¶œ
        future_to_file = {
            executor.submit(read_single_file, db_path): db_path 
            for db_path in db_files
        }
        
        # ì§„í–‰ ìƒí™© ì¶”ì 
        completed = 0
        total = len(db_files)
        
        # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(future_to_file):
            db_path, df, status = future.result()
            results[db_path] = (df, status)
            completed += 1
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            filename = os.path.basename(db_path)
            
            # ë‚¨ì€ ì‘ì—… ìˆ˜ ê³„ì‚° (ì œì¶œëœ ì‘ì—… - ì™„ë£Œëœ ì‘ì—…)
            remaining_tasks = total - completed
            
            if df is not None:
                tprint(f"  [ì™„ë£Œ] [{completed}/{total}] {filename}: {len(df):,} í–‰ (ëŒ€ê¸°: {remaining_tasks}ê°œ)")
            else:
                tprint(f"  [ì˜¤ë¥˜] [{completed}/{total}] {filename}: {status} (ëŒ€ê¸°: {remaining_tasks}ê°œ)")
    
    # ì›ë³¸ íŒŒì¼ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ë°˜í™˜
    return [results.get(db_path, (None, "ì²˜ë¦¬ ì•ˆë¨"))[0] for db_path in db_files]


# ë©”ëª¨ë¦¬ ìºì‹±ì„ ìœ„í•œ ì „ì—­ ìºì‹œ (íŒŒì¼ë³„, íŒŒë¼ë¯¸í„°ë³„)
_cache = {}
_cache_max_size = 50  # ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜


def _get_cache_key(db_path, params_to_read):
    """ìºì‹œ í‚¤ ìƒì„±"""
    params_str = ','.join(sorted(params_to_read))
    return f"{db_path}::{params_str}"


def read_db_file_with_cache(db_path, params_to_read, time_cols, convert_datetime_vectorized, 
                             use_cache=True):
    """
    ìºì‹±ì„ ì‚¬ìš©í•œ DB íŒŒì¼ ì½ê¸° (ê°™ì€ ìš”ì²­ ì¬ì‚¬ìš© ì‹œ ì¦‰ì‹œ ë°˜í™˜)
    
    Args:
        db_path: DB íŒŒì¼ ê²½ë¡œ
        params_to_read: ì½ì„ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸
        time_cols: ì‹œê°„ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        convert_datetime_vectorized: ë²¡í„°í™”ëœ datetime ë³€í™˜ í•¨ìˆ˜
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        pd.DataFrame: ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    if not use_cache:
        return read_db_file(db_path, params_to_read, time_cols, convert_datetime_vectorized)
    
    cache_key = _get_cache_key(db_path, params_to_read)
    
    # ìºì‹œ í™•ì¸
    if cache_key in _cache:
        cached_df, cached_time = _cache[cache_key]
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸ (íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ìºì‹œ ë¬´íš¨í™”)
        try:
            file_mtime = os.path.getmtime(db_path)
            if cached_time >= file_mtime:
                tprint(f"  ğŸ’¾ ìºì‹œì—ì„œ ì½ê¸°: {os.path.basename(db_path)}")
                return cached_df.copy()
        except OSError:
            pass
    
    # ìºì‹œ ë¯¸ìŠ¤: ì‹¤ì œ ì½ê¸°
    df = read_db_file(db_path, params_to_read, time_cols, convert_datetime_vectorized)
                
    if df is not None:
        # ìºì‹œì— ì €ì¥ (ìµœëŒ€ í¬ê¸° ì œí•œ)
        if len(_cache) >= _cache_max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (FIFO)
            oldest_key = next(iter(_cache))
            del _cache[oldest_key]
        
        file_mtime = os.path.getmtime(db_path) if os.path.exists(db_path) else 0
        _cache[cache_key] = (df.copy(), file_mtime)
    
    return df


def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    global _cache
    _cache.clear()
    print("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


def is_cnt_related_data(db_path, params_to_read):
    """
    ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ CNT ê´€ë ¨ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
    Args:
        db_path: ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
        params_to_read: ì½ìœ¼ë ¤ëŠ” íŒŒë¼ë¯¸í„° ëª©ë¡
    Returns:
        bool: CNT ê´€ë ¨ ë°ì´í„°ë©´ True, ì•„ë‹ˆë©´ False
    """
    # íŒŒì¼ëª… ê¸°ë°˜ ì²´í¬ (ê¸°ì¡´ ë¡œì§)
    db_filename = os.path.basename(db_path).lower()
    if 'cnt' in db_filename or 'monitoring' in db_filename:
        return True
    
    # CNT ê´€ë ¨ íŒŒë¼ë¯¸í„°ëª… íŒ¨í„´ë“¤
    cnt_patterns = [
        r'cnt\d*',  # cnt, cnt1, cnt2 ë“±
        r'cn[a-z]\d*',  # cnA, cnB, cnC ë“±
        r'count\d*',  # count, count1, count2 ë“±
        r'monitor\d*',  # monitor, monitor1 ë“±
        r'sensor\d*cnt',  # sensor1cnt ë“±
    ]
    
    # íŒŒë¼ë¯¸í„°ëª…ì—ì„œ CNT ê´€ë ¨ íŒ¨í„´ ì²´í¬
    for param in params_to_read:
        param_lower = param.lower()
        for pattern in cnt_patterns:
            if re.search(pattern, param_lower):
                print(f"CNT ê´€ë ¨ íŒŒë¼ë¯¸í„° ë°œê²¬: {param}")
                return True
    
    # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” êµ¬ì¡° ì²´í¬ (ê°„ë‹¨í•œ ìƒ˜í”Œë§)
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # í…Œì´ë¸” ëª©ë¡ í™•ì¸
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        
        for table_name in tables:
            table = table_name[0]
            # ì»¬ëŸ¼ ëª©ë¡ í™•ì¸
            cursor.execute(f"PRAGMA table_info({table});")
            columns = cursor.fetchall()
            
            for col_info in columns:
                col_name = col_info[1].lower()
                for pattern in cnt_patterns:
                    if re.search(pattern, col_name):
                        print(f"CNT ê´€ë ¨ ì»¬ëŸ¼ ë°œê²¬: {col_name} in table {table}")
                        conn.close()
                        return True
        
        conn.close()
        
    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° ì²´í¬ ì‹¤íŒ¨ {db_path}: {e}")
    
    return False


# create_onselect_function_with_contextëŠ” Onselect_integral.py ëª¨ë“ˆë¡œ ë¶„ë¦¬ë¨
# LDR í•˜ìœ„ ëª¨ë“ˆë¡œ importí•˜ì—¬ ì‚¬ìš©
from Onselect_integral import create_onselect_function_with_context
